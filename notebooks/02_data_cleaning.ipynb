{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Data Cleaning Notebook ‚Äî AquaSafe\n",
    "\n",
    "**Notebook:** `02_data_cleaning.ipynb`\n",
    "\n",
    "**Input:** `data/raw/NWMP_August2025_MPCB_0.csv`\n",
    "\n",
    "**Output:** `data/processed/cleaned_water_quality.csv` (structurally clean, may contain NaN)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Perform **structural cleaning** on raw water quality data to prepare it for feature engineering.\n",
    "\n",
    "### ‚úÖ What This Notebook Does:\n",
    "| Task | Description |\n",
    "|------|-------------|\n",
    "| Column Standardization | Normalize names to snake_case |\n",
    "| Type Normalization | Parse BDL annotations, convert to numeric |\n",
    "| Coordinate Conversion | DMS ‚Üí Decimal Degrees |\n",
    "| Target Mapping | Verbose labels ‚Üí A/B/C/E codes |\n",
    "| Column Removal | Drop leakage, metadata, identifier columns |\n",
    "| Invalid Row Removal | Remove rows with missing/invalid target |\n",
    "\n",
    "### ‚ùå What This Notebook Does NOT Do:\n",
    "| Task | Reason | Where It Happens |\n",
    "|------|--------|------------------|\n",
    "| Imputation | Must happen AFTER train-test split to avoid leakage | Notebook 03 |\n",
    "| Encoding | Must happen AFTER train-test split to avoid leakage | Notebook 03 |\n",
    "| Scaling | Part of model pipeline | Notebook 04 |\n",
    "| Train-Test Split | Belongs in feature engineering | Notebook 03 |\n",
    "\n",
    "### üí° Why This Approach?\n",
    "Imputation and encoding on full data causes **data leakage** ‚Äî test set statistics would contaminate training data, leading to overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project modules\n",
    "from utils.config import DATA_PATH, DATA_DIR\n",
    "from src.data_preprocessing.create_dataframe import create_dataframe\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Plot settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Section 2: Data Loading & Initial Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded successfully from: /Users/rex/Documents/personal/AquaSafe/data/NWMP_August2025_MPCB_0.csv\n",
      "  Shape: 222 rows √ó 54 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD RAW DATA\n",
    "# ============================================================================\n",
    "\n",
    "df = create_dataframe(DATA_PATH, encoding=\"latin-1\")\n",
    "\n",
    "print(f\"‚úì Data loaded successfully from: {DATA_PATH}\")\n",
    "print(f\"  Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Columns standardized: 54 features normalized to snake_case\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STANDARDIZE COLUMN NAMES\n",
    "# ============================================================================\n",
    "# Normalize to snake_case for consistency with Python conventions\n",
    "\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"/\", \"_\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì Columns standardized: {df.shape[1]} features normalized to snake_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Raw data backup created: (222, 54)\n"
     ]
    }
   ],
   "source": [
    "# Keep a backup of raw data for comparison\n",
    "df_raw = df.copy()\n",
    "print(f\"‚úì Raw data backup created: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Initial Missing Values (44 columns with NaN):\n",
      "use_of_water_in_down_stream      222\n",
      "remark                           215\n",
      "odor                             170\n",
      "visibility_effluent_discharge     94\n",
      "major_polluting_sources           73\n",
      "fecal_streptococci                48\n",
      "boron                             34\n",
      "temperature                       29\n",
      "river_basin                       24\n",
      "name_of_water_body                22\n",
      "flouride                          18\n",
      "magnesium_caco3                    7\n",
      "total_kjeldahl_n                   7\n",
      "amonia_n                           7\n",
      "hardness_caco3                     7\n",
      "calcium_caco3                      7\n",
      "total_fixed_solids                 7\n",
      "sulphate                           7\n",
      "sodium                             7\n",
      "total_dissolved_solids             7\n",
      "chlorides                          7\n",
      "total_suspended_solids             7\n",
      "phosphate                          7\n",
      "potassium                          7\n",
      "cod                                7\n",
      "turbidity                          7\n",
      "total_alkalinity                   7\n",
      "phenophelene_alkanity              7\n",
      "use_based_class                    7\n",
      "weather                            7\n",
      "approx_depth                       7\n",
      "human_activities                   7\n",
      "floating_matter                    7\n",
      "color                              7\n",
      "flow                               7\n",
      "dissolved_o2                       7\n",
      "ph                                 7\n",
      "conductivity                       7\n",
      "bod                                7\n",
      "nitrate_n                          7\n",
      "fecal_coliform                     7\n",
      "total_coliform                     7\n",
      "latitude                           6\n",
      "longitude                          6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initial missing values check\n",
    "missing_counts = df.isna().sum()\n",
    "missing_cols = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Initial Missing Values ({len(missing_cols)} columns with NaN):\")\n",
    "print(missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Section 3: Type Normalization (BDL Parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Identified 17 numeric columns with potential BDL annotations\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NUMERIC COLUMNS WITH BDL ANNOTATIONS\n",
    "# ============================================================================\n",
    "# Problem: Chemical/biological parameters contain \"(BDL)\" annotations\n",
    "#          meaning \"Below Detection Limit\" - lab equipment threshold\n",
    "# Solution: Extract numeric component + preserve BDL flag as binary feature\n",
    "\n",
    "NUMERIC_STRING_COLS = [\n",
    "    \"fecal_coliform\",\n",
    "    \"total_coliform\",\n",
    "    \"fecal_streptococci\",\n",
    "    \"total_kjeldahl_n\",\n",
    "    \"nitrate_n\",\n",
    "    \"turbidity\",\n",
    "    \"sulphate\",\n",
    "    \"sodium\",\n",
    "    \"chlorides\",\n",
    "    \"phosphate\",\n",
    "    \"boron\",\n",
    "    \"potassium\",\n",
    "    \"flouride\",\n",
    "    \"dissolved_o2\",\n",
    "    \"total_suspended_solids\",\n",
    "    \"phenophelene_alkanity\",\n",
    "    \"total_alkalinity\",\n",
    "]\n",
    "\n",
    "print(f\"‚úì Identified {len(NUMERIC_STRING_COLS)} numeric columns with potential BDL annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_with_bdl(series: pd.Series) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert numeric strings with BDL annotations to float while\n",
    "    preserving detection-limit information.\n",
    "\n",
    "    Args:\n",
    "        series: Column with mixed numeric/annotated values\n",
    "\n",
    "    Returns:\n",
    "        tuple: (numeric_values, is_bdl_flag)\n",
    "    \"\"\"\n",
    "    # Identify BDL presence before altering values\n",
    "    is_bdl = series.astype(str).str.contains(\"BDL\", na=False)\n",
    "\n",
    "    # Remove annotation and extract numeric portion\n",
    "    numeric = (\n",
    "        series.astype(str)\n",
    "        .str.replace(\"(BDL)\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Coerce to numeric (invalid ‚Üí NaN)\n",
    "    numeric = pd.to_numeric(numeric, errors=\"coerce\")\n",
    "\n",
    "    return numeric, is_bdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì fecal_coliform: 28 BDL flags, 7 NaN values\n",
      "  ‚úì total_coliform: 1 BDL flags, 7 NaN values\n",
      "  ‚úì fecal_streptococci: 143 BDL flags, 48 NaN values\n",
      "  ‚úì total_kjeldahl_n: 101 BDL flags, 7 NaN values\n",
      "  ‚úì nitrate_n: 20 BDL flags, 9 NaN values\n",
      "  ‚úì turbidity: 77 BDL flags, 7 NaN values\n",
      "  ‚úì sulphate: 22 BDL flags, 7 NaN values\n",
      "  ‚úì sodium: 22 BDL flags, 7 NaN values\n",
      "  ‚úì chlorides: 2 BDL flags, 7 NaN values\n",
      "  ‚úì phosphate: 101 BDL flags, 11 NaN values\n",
      "  ‚úì boron: 131 BDL flags, 34 NaN values\n",
      "  ‚úì potassium: 91 BDL flags, 7 NaN values\n",
      "  ‚úì flouride: 115 BDL flags, 18 NaN values\n",
      "  ‚úì dissolved_o2: 8 BDL flags, 7 NaN values\n",
      "  ‚úì total_suspended_solids: 40 BDL flags, 7 NaN values\n",
      "  ‚úì phenophelene_alkanity: 168 BDL flags, 10 NaN values\n",
      "  ‚úì total_alkalinity: 5 BDL flags, 7 NaN values\n",
      "\n",
      "‚úì Numeric normalization complete: 17 columns processed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# APPLY BDL PARSING\n",
    "# ============================================================================\n",
    "\n",
    "conversions_made = 0\n",
    "\n",
    "for col in NUMERIC_STRING_COLS:\n",
    "    if col not in df.columns:\n",
    "        print(f\"  ‚ö† Column not found: {col}\")\n",
    "        continue\n",
    "\n",
    "    numeric_values, bdl_flag = parse_numeric_with_bdl(df[col])\n",
    "    na_count = numeric_values.isna().sum()\n",
    "    bdl_count = bdl_flag.sum()\n",
    "\n",
    "    # Replace original column with numeric representation\n",
    "    df[col] = numeric_values\n",
    "    conversions_made += 1\n",
    "\n",
    "    # Preserve detection-limit information as binary flag\n",
    "    df[f\"{col}_is_bdl\"] = bdl_flag\n",
    "\n",
    "    print(f\"  ‚úì {col}: {bdl_count} BDL flags, {na_count} NaN values\")\n",
    "\n",
    "print(f\"\\n‚úì Numeric normalization complete: {conversions_made} columns processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üåç Section 4: Geographic Coordinate Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dms_coordinate(value) -> float:\n",
    "    \"\"\"\n",
    "    Convert geographic coordinates from Degree-Minute format to decimal degrees.\n",
    "\n",
    "    Input Format: \"19¬∞29.263'\"\n",
    "    Output: 19.487716...\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        value = str(value).replace(\"\\ufffd\", \"¬∞\")\n",
    "        degree_part, minute_part = value.split(\"¬∞\")\n",
    "\n",
    "        degrees = float(degree_part.strip())\n",
    "        minutes = float(minute_part.replace(\"'\", \"\").strip())\n",
    "\n",
    "        return degrees + (minutes / 60)\n",
    "\n",
    "    except Exception:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Coordinates standardized to decimal degrees\n",
      "  Latitude range: [16.69, 21.27]\n",
      "  Longitude range: [73.18, 79.20]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# APPLY COORDINATE CONVERSION\n",
    "# ============================================================================\n",
    "\n",
    "df[\"latitude\"] = df[\"latitude\"].apply(parse_dms_coordinate)\n",
    "df[\"longitude\"] = df[\"longitude\"].apply(parse_dms_coordinate)\n",
    "\n",
    "print(\"‚úì Coordinates standardized to decimal degrees\")\n",
    "print(f\"  Latitude range: [{df['latitude'].min():.2f}, {df['latitude'].max():.2f}]\")\n",
    "print(f\"  Longitude range: [{df['longitude'].min():.2f}, {df['longitude'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Section 5: Target Variable Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Target variable cleaned\n",
      "  Unique values: 5\n",
      "\n",
      "Value counts:\n",
      "use_based_class\n",
      "A (Drinking Water source without conventional treatment but after disinfection)    141\n",
      "No Information                                                                      44\n",
      "E (Irrigation, industrial cooling and controlled waste)                             19\n",
      "NaN                                                                                  7\n",
      "C (Drinking water source)                                                            6\n",
      "B (Outdoor bathing(Organized))                                                       5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TARGET VARIABLE STANDARDIZATION\n",
    "# ============================================================================\n",
    "\n",
    "TARGET_COL = \"use_based_class\"\n",
    "\n",
    "# Clean string representation\n",
    "df[TARGET_COL] = (\n",
    "    df[TARGET_COL]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace(\"nan\", np.nan)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Target variable cleaned\")\n",
    "print(f\"  Unique values: {df[TARGET_COL].nunique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Target mapping applied\n",
      "\n",
      "Value counts after mapping:\n",
      "use_based_class\n",
      "A      141\n",
      "NaN     51\n",
      "E       19\n",
      "C        6\n",
      "B        5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TARGET CLASS MAPPING\n",
    "# ============================================================================\n",
    "# Map verbose regulatory descriptions to compact codes\n",
    "\n",
    "TARGET_MAP = {\n",
    "    \"A (Drinking Water source without conventional treatment but after disinfection)\": \"A\",\n",
    "    \"B (Outdoor bathing(Organized))\": \"B\",\n",
    "    \"C (Drinking water source)\": \"C\",\n",
    "    \"E (Irrigation, industrial cooling and controlled waste)\": \"E\",\n",
    "    \"No Information\": np.nan,  # Will be removed\n",
    "}\n",
    "\n",
    "df[TARGET_COL] = df[TARGET_COL].replace(TARGET_MAP)\n",
    "\n",
    "print(f\"‚úì Target mapping applied\")\n",
    "print(f\"\\nValue counts after mapping:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Removed rows with invalid target\n",
      "  Before: 222 rows\n",
      "  After: 171 rows\n",
      "  Removed: 51 rows\n",
      "\n",
      "Final target distribution:\n",
      "use_based_class\n",
      "A    141\n",
      "B      5\n",
      "C      6\n",
      "E     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# REMOVE ROWS WITH INVALID TARGET\n",
    "# ============================================================================\n",
    "\n",
    "rows_before = len(df)\n",
    "df = df.dropna(subset=[TARGET_COL]).reset_index(drop=True)\n",
    "rows_after = len(df)\n",
    "\n",
    "print(f\"‚úì Removed rows with invalid target\")\n",
    "print(f\"  Before: {rows_before} rows\")\n",
    "print(f\"  After: {rows_after} rows\")\n",
    "print(f\"  Removed: {rows_before - rows_after} rows\")\n",
    "print(f\"\\nFinal target distribution:\")\n",
    "print(df[TARGET_COL].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üö´ Section 6: Column Removal (Leakage, Metadata, Identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns marked for removal: 17\n",
      "  ‚Ä¢ use_of_water_in_down_stream\n",
      "  ‚Ä¢ remark\n",
      "  ‚Ä¢ sampling_date\n",
      "  ‚Ä¢ sampling_time\n",
      "  ‚Ä¢ month\n",
      "  ‚Ä¢ state_name\n",
      "  ‚Ä¢ mon_agency\n",
      "  ‚Ä¢ frequency\n",
      "  ‚Ä¢ major_polluting_sources\n",
      "  ‚Ä¢ visibility_effluent_discharge\n",
      "  ‚Ä¢ stn_code\n",
      "  ‚Ä¢ stn_name\n",
      "  ‚Ä¢ name_of_water_body\n",
      "  ‚Ä¢ district\n",
      "  ‚Ä¢ river_basin\n",
      "  ‚Ä¢ latitude\n",
      "  ‚Ä¢ longitude\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COLUMNS TO REMOVE\n",
    "# ============================================================================\n",
    "\n",
    "COLUMNS_TO_DROP = [\n",
    "    # ‚îÄ‚îÄ Structural Issues (High Missing) ‚îÄ‚îÄ\n",
    "    \"use_of_water_in_down_stream\",  # ~77% missing\n",
    "    \"remark\",                        # ~77% missing, free text\n",
    "\n",
    "    # ‚îÄ‚îÄ Metadata (Not Water Quality Features) ‚îÄ‚îÄ\n",
    "    \"sampling_date\",\n",
    "    \"sampling_time\",\n",
    "    \"month\",\n",
    "    \"state_name\",\n",
    "    \"mon_agency\",\n",
    "    \"frequency\",\n",
    "\n",
    "    # ‚îÄ‚îÄ Data Leakage Risk ‚îÄ‚îÄ\n",
    "    \"major_polluting_sources\",       # Near-deterministic with target\n",
    "    \"visibility_effluent_discharge\", # Highly correlated with classification\n",
    "\n",
    "    # ‚îÄ‚îÄ Identifiers (No Predictive Signal) ‚îÄ‚îÄ\n",
    "    \"stn_code\",\n",
    "    \"stn_name\",\n",
    "    \"name_of_water_body\",\n",
    "    \"district\",\n",
    "    \"river_basin\",\n",
    "\n",
    "    # ‚îÄ‚îÄ Geographic (Exclude from modeling, keep for analysis) ‚îÄ‚îÄ\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "]\n",
    "\n",
    "print(f\"Columns marked for removal: {len(COLUMNS_TO_DROP)}\")\n",
    "for col in COLUMNS_TO_DROP:\n",
    "    print(f\"  ‚Ä¢ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Column removal complete\n",
      "  Before: 71 columns\n",
      "  After: 54 columns\n",
      "  Removed: 17 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DROP COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "cols_before = df.shape[1]\n",
    "df = df.drop(columns=COLUMNS_TO_DROP, errors=\"ignore\")\n",
    "cols_after = df.shape[1]\n",
    "\n",
    "print(f\"\\n‚úì Column removal complete\")\n",
    "print(f\"  Before: {cols_before} columns\")\n",
    "print(f\"  After: {cols_after} columns\")\n",
    "print(f\"  Removed: {cols_before - cols_after} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Section 7: Data Quality Check (Pre-Export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Missing Values Summary (will be imputed after split):\n",
      "   Columns with NaN: 8\n",
      "   Total NaN cells: 223\n",
      "\n",
      "Columns with missing values:\n",
      "   odor: 133 (77.8%)\n",
      "   fecal_streptococci: 30 (17.5%)\n",
      "   temperature: 22 (12.9%)\n",
      "   boron: 18 (10.5%)\n",
      "   flouride: 11 (6.4%)\n",
      "   phosphate: 4 (2.3%)\n",
      "   phenophelene_alkanity: 3 (1.8%)\n",
      "   nitrate_n: 2 (1.2%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# REMAINING MISSING VALUES\n",
    "# ============================================================================\n",
    "# Note: These will be imputed in Notebook 03 AFTER train-test split\n",
    "\n",
    "missing_counts = df.isna().sum()\n",
    "missing_cols = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"üìä Missing Values Summary (will be imputed after split):\")\n",
    "print(f\"   Columns with NaN: {len(missing_cols)}\")\n",
    "print(f\"   Total NaN cells: {missing_counts.sum()}\")\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nColumns with missing values:\")\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"   {col}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Column Type Summary:\n",
      "   Numeric columns: 29\n",
      "   Categorical columns: 8\n",
      "   Total columns: 54\n",
      "\n",
      "Categorical columns:\n",
      "   ‚Ä¢ type_water_body: 7 unique values\n",
      "   ‚Ä¢ use_based_class: 4 unique values\n",
      "   ‚Ä¢ weather: 3 unique values\n",
      "   ‚Ä¢ approx_depth: 3 unique values\n",
      "   ‚Ä¢ human_activities: 18 unique values\n",
      "   ‚Ä¢ floating_matter: 2 unique values\n",
      "   ‚Ä¢ color: 9 unique values\n",
      "   ‚Ä¢ odor: 5 unique values\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COLUMN TYPE SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìä Column Type Summary:\")\n",
    "print(f\"   Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"   Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"   Total columns: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nCategorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"   ‚Ä¢ {col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validation Checks:\n",
      "   ‚úì Dataset has 171 rows\n",
      "   ‚úì Target column complete (no NaN)\n",
      "   ‚úì Target classes valid: ['A', 'B', 'C', 'E']\n",
      "   ‚úì Duplicate rows: 0\n",
      "\n",
      "‚úÖ All validation checks passed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VALIDATION CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç Validation Checks:\")\n",
    "\n",
    "# Check 1: Dataset not empty\n",
    "assert df.shape[0] > 0, \"Dataset is empty\"\n",
    "print(f\"   ‚úì Dataset has {df.shape[0]} rows\")\n",
    "\n",
    "# Check 2: Target column exists and is complete\n",
    "assert TARGET_COL in df.columns, \"Target column missing\"\n",
    "assert df[TARGET_COL].isna().sum() == 0, \"Target has missing values\"\n",
    "print(f\"   ‚úì Target column complete (no NaN)\")\n",
    "\n",
    "# Check 3: Valid target classes\n",
    "valid_classes = {\"A\", \"B\", \"C\", \"E\"}\n",
    "actual_classes = set(df[TARGET_COL].unique())\n",
    "assert actual_classes.issubset(valid_classes), f\"Invalid classes: {actual_classes - valid_classes}\"\n",
    "print(f\"   ‚úì Target classes valid: {sorted(actual_classes)}\")\n",
    "\n",
    "# Check 4: No duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "print(f\"   ‚úì Duplicate rows: {dup_count}\")\n",
    "\n",
    "print(\"\\n‚úÖ All validation checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Section 8: Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Output directories ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE OUTPUT DIRECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "csv_folder = os.path.join(DATA_DIR, \"processed\", \"csv\")\n",
    "parquet_folder = os.path.join(DATA_DIR, \"processed\", \"parquet\")\n",
    "\n",
    "Path(csv_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(parquet_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Output directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported: /Users/rex/Documents/personal/AquaSafe/data/processed/csv/cleaned_water_quality.csv\n",
      "‚úì Exported: /Users/rex/Documents/personal/AquaSafe/data/processed/parquet/cleaned_water_quality.parquet\n",
      "\n",
      "üìä Export Summary:\n",
      "   Rows: 171\n",
      "   Columns: 54\n",
      "   File size (CSV): 50.0 KB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPORT CLEANED DATA\n",
    "# ============================================================================\n",
    "# Note: This file contains NaN values - imputation happens after split\n",
    "\n",
    "OUTPUT_CSV = os.path.join(csv_folder, \"cleaned_water_quality.csv\")\n",
    "OUTPUT_PARQUET = os.path.join(parquet_folder, \"cleaned_water_quality.parquet\")\n",
    "\n",
    "# Export CSV\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"‚úì Exported: {OUTPUT_CSV}\")\n",
    "\n",
    "# Export Parquet\n",
    "df.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "print(f\"‚úì Exported: {OUTPUT_PARQUET}\")\n",
    "\n",
    "print(f\"\\nüìä Export Summary:\")\n",
    "print(f\"   Rows: {df.shape[0]}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")\n",
    "print(f\"   File size (CSV): {os.path.getsize(OUTPUT_CSV) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Section 9: Cleaning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã DATA CLEANING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "TRANSFORMATIONS APPLIED:\n",
      "------------------------\n",
      "1. Column names standardized to snake_case\n",
      "2. BDL annotations parsed: 17 columns ‚Üí numeric + binary flags\n",
      "3. Coordinates converted: DMS ‚Üí Decimal Degrees\n",
      "4. Target mapped: Verbose labels ‚Üí A/B/C/E codes\n",
      "5. Invalid target rows removed: 51 rows\n",
      "6. Problematic columns dropped: 17 columns\n",
      "\n",
      "OUTPUT DATASET:\n",
      "---------------\n",
      "‚Ä¢ Rows: 171\n",
      "‚Ä¢ Columns: 54\n",
      "‚Ä¢ Numeric features: 29\n",
      "‚Ä¢ Categorical features: 8\n",
      "‚Ä¢ Missing values: 223 (will be imputed after split)\n",
      "\n",
      "IMPORTANT NOTES:\n",
      "----------------\n",
      "‚ö†Ô∏è This file contains NaN values intentionally\n",
      "‚ö†Ô∏è Imputation will happen in Notebook 03 AFTER train-test split\n",
      "‚ö†Ô∏è This prevents data leakage from test set to training set\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Data cleaning complete - Ready for feature engineering\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLEANING TRANSFORMATION SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "TRANSFORMATIONS APPLIED:\n",
    "------------------------\n",
    "1. Column names standardized to snake_case\n",
    "2. BDL annotations parsed: {len(NUMERIC_STRING_COLS)} columns ‚Üí numeric + binary flags\n",
    "3. Coordinates converted: DMS ‚Üí Decimal Degrees\n",
    "4. Target mapped: Verbose labels ‚Üí A/B/C/E codes\n",
    "5. Invalid target rows removed: {rows_before - rows_after} rows\n",
    "6. Problematic columns dropped: {len(COLUMNS_TO_DROP)} columns\n",
    "\n",
    "OUTPUT DATASET:\n",
    "---------------\n",
    "‚Ä¢ Rows: {df.shape[0]}\n",
    "‚Ä¢ Columns: {df.shape[1]}\n",
    "‚Ä¢ Numeric features: {len(numeric_cols)}\n",
    "‚Ä¢ Categorical features: {len(categorical_cols)}\n",
    "‚Ä¢ Missing values: {df.isna().sum().sum()} (will be imputed after split)\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "----------------\n",
    "‚ö†Ô∏è This file contains NaN values intentionally\n",
    "‚ö†Ô∏è Imputation will happen in Notebook 03 AFTER train-test split\n",
    "‚ö†Ô∏è This prevents data leakage from test set to training set\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Data cleaning complete - Ready for feature engineering\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
