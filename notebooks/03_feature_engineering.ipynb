{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Feature Engineering Notebook ‚Äî AquaSafe\n",
    "\n",
    "**Notebook:** `03_feature_engineering.ipynb`\n",
    "\n",
    "**Input:** `data/processed/cleaned_water_quality.csv` (from Notebook 02)\n",
    "\n",
    "**Output:** `data/processed/train.csv`, `data/processed/test.csv` (model-ready, no NaN)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Transform cleaned data into **model-ready train/test datasets** with proper handling to avoid data leakage.\n",
    "\n",
    "### ‚úÖ What This Notebook Does (In Order):\n",
    "| Step | Task | Why This Order? |\n",
    "|------|------|----------------|\n",
    "| 1 | Load cleaned data | Start point |\n",
    "| 2 | **Train-Test Split** | MUST happen before any transformation |\n",
    "| 3 | Imputation (fit on train) | Prevents test statistics leaking to train |\n",
    "| 4 | Encoding (fit on train) | Prevents test categories leaking to train |\n",
    "| 5 | Export train/test separately | Ready for modeling |\n",
    "\n",
    "### üí° Why Split First?\n",
    "If we impute or encode on full data:\n",
    "- Median/mode values include test set information\n",
    "- Encoder sees categories from test set\n",
    "- This causes **data leakage** ‚Üí overly optimistic metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Section 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn - Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Project modules\n",
    "from utils.config import DATA_DIR\n",
    "from src.data_preprocessing.create_dataframe import create_dataframe\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Section 2: Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded cleaned dataset: (171, 54)\n",
      "  Rows: 171\n",
      "  Columns: 54\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA FROM NOTEBOOK 02\n",
    "# ============================================================================\n",
    "\n",
    "INPUT_PATH = os.path.join(DATA_DIR, \"processed\", \"csv\", \"cleaned_water_quality.csv\")\n",
    "\n",
    "df = create_dataframe(INPUT_PATH)\n",
    "\n",
    "print(f\"‚úì Loaded cleaned dataset: {df.shape}\")\n",
    "print(f\"  Rows: {df.shape[0]}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Input Validation:\n",
      "   ‚úì Target column present\n",
      "   ‚úì Target has no missing values\n",
      "   ‚úì Target classes: ['A', 'B', 'C', 'E']\n",
      "   ‚Ñπ Missing values: 223 (will be imputed after split)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INPUT VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç Input Validation:\")\n",
    "\n",
    "TARGET_COL = \"use_based_class\"\n",
    "\n",
    "# Check target exists\n",
    "assert TARGET_COL in df.columns, f\"Target column '{TARGET_COL}' not found\"\n",
    "print(f\"   ‚úì Target column present\")\n",
    "\n",
    "# Check target is complete\n",
    "assert df[TARGET_COL].isna().sum() == 0, \"Target has missing values\"\n",
    "print(f\"   ‚úì Target has no missing values\")\n",
    "\n",
    "# Check valid classes\n",
    "valid_classes = {\"A\", \"B\", \"C\", \"E\"}\n",
    "actual_classes = set(df[TARGET_COL].unique())\n",
    "assert actual_classes.issubset(valid_classes), f\"Invalid classes found\"\n",
    "print(f\"   ‚úì Target classes: {sorted(actual_classes)}\")\n",
    "\n",
    "# Show missing values (expected - will be imputed)\n",
    "missing_count = df.isna().sum().sum()\n",
    "print(f\"   ‚Ñπ Missing values: {missing_count} (will be imputed after split)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÇÔ∏è Section 3: Feature-Target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Features and target separated:\n",
      "   X shape: (171, 53)\n",
      "   y shape: (171,)\n",
      "\n",
      "Target distribution:\n",
      "use_based_class\n",
      "A    141\n",
      "B      5\n",
      "C      6\n",
      "E     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SEPARATE FEATURES AND TARGET\n",
    "# ============================================================================\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "print(f\"‚úì Features and target separated:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Column Types:\n",
      "   Numeric columns: 29\n",
      "   Categorical columns: 7\n",
      "\n",
      "Categorical columns to encode:\n",
      "   ‚Ä¢ type_water_body: 7 unique values\n",
      "   ‚Ä¢ weather: 3 unique values\n",
      "   ‚Ä¢ approx_depth: 3 unique values\n",
      "   ‚Ä¢ human_activities: 18 unique values\n",
      "   ‚Ä¢ floating_matter: 2 unique values\n",
      "   ‚Ä¢ color: 9 unique values\n",
      "   ‚Ä¢ odor: 5 unique values\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY COLUMN TYPES\n",
    "# ============================================================================\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìä Column Types:\")\n",
    "print(f\"   Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"   Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    print(f\"\\nCategorical columns to encode:\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"   ‚Ä¢ {col}: {X[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÄ Section 4: Train-Test Split (BEFORE Any Transformation)\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: This must happen BEFORE imputation and encoding!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Train-Test Split Complete (stratified):\n",
      "   X_train: (136, 53)\n",
      "   X_test:  (35, 53)\n",
      "   y_train: (136,)\n",
      "   y_test:  (35,)\n",
      "\n",
      "Class distribution in train:\n",
      "use_based_class\n",
      "A    112\n",
      "B      4\n",
      "C      5\n",
      "E     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test:\n",
      "use_based_class\n",
      "A    29\n",
      "B     1\n",
      "C     1\n",
      "E     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "# This MUST happen before any imputation or encoding to prevent data leakage\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train-Test Split Complete (stratified):\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_test:  {X_test.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   y_test:  {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution in train:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nClass distribution in test:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Stratification Verification:\n",
      "   Class A: Train=82.4%, Test=82.9%\n",
      "   Class B: Train=2.9%, Test=2.9%\n",
      "   Class C: Train=3.7%, Test=2.9%\n",
      "   Class E: Train=11.0%, Test=11.4%\n",
      "\n",
      "   ‚úì Class proportions preserved in both sets\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFY STRATIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç Stratification Verification:\")\n",
    "\n",
    "train_pct = y_train.value_counts(normalize=True).sort_index() * 100\n",
    "test_pct = y_test.value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "for cls in sorted(y.unique()):\n",
    "    print(f\"   Class {cls}: Train={train_pct[cls]:.1f}%, Test={test_pct[cls]:.1f}%\")\n",
    "\n",
    "print(\"\\n   ‚úì Class proportions preserved in both sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Section 5: Imputation (Fit on Train Only)\n",
    "\n",
    "**Strategy:**\n",
    "- Numeric columns: Median imputation (robust to outliers)\n",
    "- Categorical columns: Most frequent (mode) imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Missing Values Before Imputation:\n",
      "\n",
      "   Train set: 176 missing values\n",
      "   Test set:  47 missing values\n",
      "\n",
      "   Columns with missing values (train):\n",
      "      ‚Ä¢ odor: 106 (77.9%)\n",
      "      ‚Ä¢ fecal_streptococci: 24 (17.6%)\n",
      "      ‚Ä¢ temperature: 16 (11.8%)\n",
      "      ‚Ä¢ boron: 13 (9.6%)\n",
      "      ‚Ä¢ flouride: 10 (7.4%)\n",
      "      ‚Ä¢ phosphate: 4 (2.9%)\n",
      "      ‚Ä¢ phenophelene_alkanity: 2 (1.5%)\n",
      "      ‚Ä¢ nitrate_n: 1 (0.7%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK MISSING VALUES BEFORE IMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìä Missing Values Before Imputation:\")\n",
    "print(f\"\\n   Train set: {X_train.isna().sum().sum()} missing values\")\n",
    "print(f\"   Test set:  {X_test.isna().sum().sum()} missing values\")\n",
    "\n",
    "# Show columns with missing values in train\n",
    "train_missing = X_train.isna().sum()\n",
    "train_missing = train_missing[train_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(train_missing) > 0:\n",
    "    print(f\"\\n   Columns with missing values (train):\")\n",
    "    for col, count in train_missing.head(10).items():\n",
    "        pct = count / len(X_train) * 100\n",
    "        print(f\"      ‚Ä¢ {col}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Numeric Imputation (Median):\n",
      "   ‚úì Imputed 29 numeric columns\n",
      "   ‚úì Fitted on train, transformed both train and test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NUMERIC IMPUTATION (MEDIAN)\n",
    "# ============================================================================\n",
    "# Fit on TRAIN only, transform both train and test\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nüîß Numeric Imputation (Median):\")\n",
    "    \n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # Fit on train only\n",
    "    numeric_imputer.fit(X_train[numeric_cols])\n",
    "    \n",
    "    # Transform both\n",
    "    X_train[numeric_cols] = numeric_imputer.transform(X_train[numeric_cols])\n",
    "    X_test[numeric_cols] = numeric_imputer.transform(X_test[numeric_cols])\n",
    "    \n",
    "    print(f\"   ‚úì Imputed {len(numeric_cols)} numeric columns\")\n",
    "    print(f\"   ‚úì Fitted on train, transformed both train and test\")\n",
    "else:\n",
    "    numeric_imputer = None\n",
    "    print(\"   ‚Ñπ No numeric columns to impute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Categorical Imputation (Most Frequent):\n",
      "   ‚úì Imputed 7 categorical columns\n",
      "   ‚úì Fitted on train, transformed both train and test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CATEGORICAL IMPUTATION (MOST FREQUENT)\n",
    "# ============================================================================\n",
    "# Fit on TRAIN only, transform both train and test\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nüîß Categorical Imputation (Most Frequent):\")\n",
    "    \n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Fit on train only\n",
    "    categorical_imputer.fit(X_train[categorical_cols])\n",
    "    \n",
    "    # Transform both\n",
    "    X_train[categorical_cols] = categorical_imputer.transform(X_train[categorical_cols])\n",
    "    X_test[categorical_cols] = categorical_imputer.transform(X_test[categorical_cols])\n",
    "    \n",
    "    print(f\"   ‚úì Imputed {len(categorical_cols)} categorical columns\")\n",
    "    print(f\"   ‚úì Fitted on train, transformed both train and test\")\n",
    "else:\n",
    "    categorical_imputer = None\n",
    "    print(\"   ‚Ñπ No categorical columns to impute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Missing Values After Imputation:\n",
      "   Train set: 0 missing values\n",
      "   Test set:  0 missing values\n",
      "   ‚úì All missing values imputed successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFY NO MISSING VALUES AFTER IMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Missing Values After Imputation:\")\n",
    "print(f\"   Train set: {X_train.isna().sum().sum()} missing values\")\n",
    "print(f\"   Test set:  {X_test.isna().sum().sum()} missing values\")\n",
    "\n",
    "assert X_train.isna().sum().sum() == 0, \"Train still has missing values!\"\n",
    "assert X_test.isna().sum().sum() == 0, \"Test still has missing values!\"\n",
    "\n",
    "print(\"   ‚úì All missing values imputed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üè∑Ô∏è Section 6: One-Hot Encoding (Fit on Train Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è One-Hot Encoding:\n",
      "   ‚úì Encoded 7 categorical columns\n",
      "   ‚úì Created 46 dummy columns\n",
      "   ‚úì Fitted on train, transformed both train and test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ONE-HOT ENCODING\n",
    "# ============================================================================\n",
    "# Fit on TRAIN only, transform both train and test\n",
    "# handle_unknown='ignore' ensures unseen categories in test don't cause errors\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nüè∑Ô∏è One-Hot Encoding:\")\n",
    "    \n",
    "    encoder = OneHotEncoder(\n",
    "        sparse_output=False,\n",
    "        handle_unknown='ignore',  # Important: handles unseen categories in test\n",
    "        drop=None  # Keep all categories\n",
    "    )\n",
    "    \n",
    "    # Fit on train only\n",
    "    encoder.fit(X_train[categorical_cols])\n",
    "    \n",
    "    # Get feature names\n",
    "    encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Transform both\n",
    "    train_encoded = pd.DataFrame(\n",
    "        encoder.transform(X_train[categorical_cols]),\n",
    "        columns=encoded_feature_names,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    test_encoded = pd.DataFrame(\n",
    "        encoder.transform(X_test[categorical_cols]),\n",
    "        columns=encoded_feature_names,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Drop original categorical columns and add encoded ones\n",
    "    X_train = X_train.drop(columns=categorical_cols)\n",
    "    X_test = X_test.drop(columns=categorical_cols)\n",
    "    \n",
    "    X_train = pd.concat([X_train, train_encoded], axis=1)\n",
    "    X_test = pd.concat([X_test, test_encoded], axis=1)\n",
    "    \n",
    "    print(f\"   ‚úì Encoded {len(categorical_cols)} categorical columns\")\n",
    "    print(f\"   ‚úì Created {len(encoded_feature_names)} dummy columns\")\n",
    "    print(f\"   ‚úì Fitted on train, transformed both train and test\")\n",
    "else:\n",
    "    encoder = None\n",
    "    print(\"   ‚Ñπ No categorical columns to encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è Target Encoding:\n",
      "   Classes: ['A' 'B' 'C' 'E']\n",
      "   Encoded: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENCODE TARGET VARIABLE\n",
    "# ============================================================================\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit on all possible classes (A, B, C, E)\n",
    "label_encoder.fit(['A', 'B', 'C', 'E'])\n",
    "\n",
    "# Transform\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Target Encoding:\")\n",
    "print(f\"   Classes: {label_encoder.classes_}\")\n",
    "print(f\"   Encoded: {list(range(len(label_encoder.classes_)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Section 7: Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Converting 17 boolean columns to integer:\n",
      "   ‚úì Converted: ['fecal_coliform_is_bdl', 'total_coliform_is_bdl', 'fecal_streptococci_is_bdl']... (and 14 more)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONVERT BOOLEAN COLUMNS TO INTEGER\n",
    "# ============================================================================\n",
    "# BDL flag columns are boolean (True/False) - convert to int (0/1)\n",
    "\n",
    "bool_cols_train = X_train.select_dtypes(include=['bool']).columns.tolist()\n",
    "bool_cols_test = X_test.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "if len(bool_cols_train) > 0:\n",
    "    print(f\"\\nüîß Converting {len(bool_cols_train)} boolean columns to integer:\")\n",
    "    for col in bool_cols_train:\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "    for col in bool_cols_test:\n",
    "        X_test[col] = X_test[col].astype(int)\n",
    "    print(f\"   ‚úì Converted: {bool_cols_train[:3]}... (and {len(bool_cols_train)-3} more)\")\n",
    "else:\n",
    "    print(\"\\n   ‚Ñπ No boolean columns to convert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Dataset Validation:\n",
      "============================================================\n",
      "\n",
      "   Train set: 136 rows √ó 92 features\n",
      "   Test set:  35 rows √ó 92 features\n",
      "   ‚úì Train and test have identical columns\n",
      "   ‚úì No missing values in train or test\n",
      "   ‚úì X and y aligned correctly\n",
      "   ‚úì All features are numeric\n",
      "\n",
      "============================================================\n",
      "‚úÖ All validation checks passed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL DATASET VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç Final Dataset Validation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Shape check\n",
    "print(f\"\\n   Train set: {X_train.shape[0]} rows √ó {X_train.shape[1]} features\")\n",
    "print(f\"   Test set:  {X_test.shape[0]} rows √ó {X_test.shape[1]} features\")\n",
    "\n",
    "# Column alignment check\n",
    "assert list(X_train.columns) == list(X_test.columns), \"Column mismatch!\"\n",
    "print(f\"   ‚úì Train and test have identical columns\")\n",
    "\n",
    "# No missing values\n",
    "assert X_train.isna().sum().sum() == 0, \"Train has NaN!\"\n",
    "assert X_test.isna().sum().sum() == 0, \"Test has NaN!\"\n",
    "print(f\"   ‚úì No missing values in train or test\")\n",
    "\n",
    "# Row alignment\n",
    "assert len(X_train) == len(y_train_encoded), \"Train X/y mismatch!\"\n",
    "assert len(X_test) == len(y_test_encoded), \"Test X/y mismatch!\"\n",
    "print(f\"   ‚úì X and y aligned correctly\")\n",
    "\n",
    "# All numeric\n",
    "non_numeric = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "assert len(non_numeric) == 0, f\"Non-numeric columns found: {list(non_numeric)}\"\n",
    "print(f\"   ‚úì All features are numeric\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All validation checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Section 8: Export Train/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Final dataframes prepared:\n",
      "   Train: (136, 93)\n",
      "   Test:  (35, 93)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPARE FINAL DATAFRAMES\n",
    "# ============================================================================\n",
    "\n",
    "# Add target back to dataframes for export\n",
    "train_df = X_train.copy()\n",
    "train_df[TARGET_COL] = y_train_encoded\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df[TARGET_COL] = y_test_encoded\n",
    "\n",
    "print(f\"‚úì Final dataframes prepared:\")\n",
    "print(f\"   Train: {train_df.shape}\")\n",
    "print(f\"   Test:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Output directories ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE OUTPUT DIRECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "csv_folder = os.path.join(DATA_DIR, \"processed\", \"csv\")\n",
    "parquet_folder = os.path.join(DATA_DIR, \"processed\", \"parquet\")\n",
    "models_folder = os.path.join(Path(DATA_DIR).parent, \"models\")\n",
    "\n",
    "Path(csv_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(parquet_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(models_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Output directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported: /Users/rex/Documents/personal/AquaSafe/data/processed/csv/train.csv\n",
      "‚úì Exported: /Users/rex/Documents/personal/AquaSafe/data/processed/csv/test.csv\n",
      "‚úì Exported: /Users/rex/Documents/personal/AquaSafe/data/processed/parquet/train.parquet\n",
      "‚úì Exported: /Users/rex/Documents/personal/AquaSafe/data/processed/parquet/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPORT TRAIN/TEST DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "# CSV exports\n",
    "TRAIN_CSV = os.path.join(csv_folder, \"train.csv\")\n",
    "TEST_CSV = os.path.join(csv_folder, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "print(f\"‚úì Exported: {TRAIN_CSV}\")\n",
    "print(f\"‚úì Exported: {TEST_CSV}\")\n",
    "\n",
    "# Parquet exports\n",
    "TRAIN_PARQUET = os.path.join(parquet_folder, \"train.parquet\")\n",
    "TEST_PARQUET = os.path.join(parquet_folder, \"test.parquet\")\n",
    "\n",
    "train_df.to_parquet(TRAIN_PARQUET, index=False)\n",
    "test_df.to_parquet(TEST_PARQUET, index=False)\n",
    "\n",
    "print(f\"‚úì Exported: {TRAIN_PARQUET}\")\n",
    "print(f\"‚úì Exported: {TEST_PARQUET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: numeric_imputer.pkl\n",
      "‚úì Saved: categorical_imputer.pkl\n",
      "‚úì Saved: onehot_encoder.pkl\n",
      "‚úì Saved: label_encoder.pkl\n",
      "‚úì Saved: feature_names.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE PREPROCESSORS FOR DEPLOYMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Save imputers\n",
    "if numeric_imputer is not None:\n",
    "    joblib.dump(numeric_imputer, os.path.join(models_folder, \"numeric_imputer.pkl\"))\n",
    "    print(f\"‚úì Saved: numeric_imputer.pkl\")\n",
    "\n",
    "if categorical_imputer is not None:\n",
    "    joblib.dump(categorical_imputer, os.path.join(models_folder, \"categorical_imputer.pkl\"))\n",
    "    print(f\"‚úì Saved: categorical_imputer.pkl\")\n",
    "\n",
    "# Save encoder\n",
    "if encoder is not None:\n",
    "    joblib.dump(encoder, os.path.join(models_folder, \"onehot_encoder.pkl\"))\n",
    "    print(f\"‚úì Saved: onehot_encoder.pkl\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, os.path.join(models_folder, \"label_encoder.pkl\"))\n",
    "print(f\"‚úì Saved: label_encoder.pkl\")\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(list(X_train.columns), os.path.join(models_folder, \"feature_names.pkl\"))\n",
    "print(f\"‚úì Saved: feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Feature registry saved: /Users/rex/Documents/personal/AquaSafe/data/processed/feature_registry.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE FEATURE REGISTRY\n",
    "# ============================================================================\n",
    "\n",
    "feature_registry = {\n",
    "    \"metadata\": {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": \"03_feature_engineering.ipynb\"\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_records\": len(train_df),\n",
    "        \"test_records\": len(test_df),\n",
    "        \"total_features\": len(X_train.columns),\n",
    "        \"target_name\": TARGET_COL,\n",
    "        \"target_classes\": list(label_encoder.classes_)\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"numeric_imputation\": \"median\",\n",
    "        \"categorical_imputation\": \"most_frequent\",\n",
    "        \"encoding\": \"one_hot\",\n",
    "        \"split_ratio\": \"80/20\",\n",
    "        \"stratified\": True,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    \"feature_names\": list(X_train.columns)\n",
    "}\n",
    "\n",
    "registry_path = os.path.join(DATA_DIR, \"processed\", \"feature_registry.json\")\n",
    "with open(registry_path, 'w') as f:\n",
    "    json.dump(feature_registry, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Feature registry saved: {registry_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Section 9: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "PIPELINE EXECUTED (In Order):\n",
      "-----------------------------\n",
      "1. Loaded cleaned data from Notebook 02\n",
      "2. Split data: 80% train / 20% test (stratified)\n",
      "3. Imputed numeric columns (median) - fitted on train only\n",
      "4. Imputed categorical columns (mode) - fitted on train only\n",
      "5. One-hot encoded categorical columns - fitted on train only\n",
      "6. Encoded target variable (LabelEncoder)\n",
      "\n",
      "OUTPUT DATASETS:\n",
      "----------------\n",
      "‚Ä¢ Train: 136 rows √ó 93 columns\n",
      "‚Ä¢ Test:  35 rows √ó 93 columns\n",
      "‚Ä¢ Features: 92\n",
      "‚Ä¢ Target classes: [np.str_('A'), np.str_('B'), np.str_('C'), np.str_('E')]\n",
      "\n",
      "SAVED ARTIFACTS:\n",
      "----------------\n",
      "‚Ä¢ train.csv / train.parquet\n",
      "‚Ä¢ test.csv / test.parquet\n",
      "‚Ä¢ numeric_imputer.pkl\n",
      "‚Ä¢ categorical_imputer.pkl (if applicable)\n",
      "‚Ä¢ onehot_encoder.pkl (if applicable)\n",
      "‚Ä¢ label_encoder.pkl\n",
      "‚Ä¢ feature_names.pkl\n",
      "‚Ä¢ feature_registry.json\n",
      "\n",
      "DATA LEAKAGE PREVENTION:\n",
      "------------------------\n",
      "‚úì Train-test split done BEFORE any transformation\n",
      "‚úì Imputers fitted on train only, transformed both\n",
      "‚úì Encoder fitted on train only, transformed both\n",
      "‚úì No test set information leaked to training data\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Feature engineering complete - Ready for model training\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "PIPELINE EXECUTED (In Order):\n",
    "-----------------------------\n",
    "1. Loaded cleaned data from Notebook 02\n",
    "2. Split data: 80% train / 20% test (stratified)\n",
    "3. Imputed numeric columns (median) - fitted on train only\n",
    "4. Imputed categorical columns (mode) - fitted on train only\n",
    "5. One-hot encoded categorical columns - fitted on train only\n",
    "6. Encoded target variable (LabelEncoder)\n",
    "\n",
    "OUTPUT DATASETS:\n",
    "----------------\n",
    "‚Ä¢ Train: {len(train_df)} rows √ó {train_df.shape[1]} columns\n",
    "‚Ä¢ Test:  {len(test_df)} rows √ó {test_df.shape[1]} columns\n",
    "‚Ä¢ Features: {len(X_train.columns)}\n",
    "‚Ä¢ Target classes: {list(label_encoder.classes_)}\n",
    "\n",
    "SAVED ARTIFACTS:\n",
    "----------------\n",
    "‚Ä¢ train.csv / train.parquet\n",
    "‚Ä¢ test.csv / test.parquet\n",
    "‚Ä¢ numeric_imputer.pkl\n",
    "‚Ä¢ categorical_imputer.pkl (if applicable)\n",
    "‚Ä¢ onehot_encoder.pkl (if applicable)\n",
    "‚Ä¢ label_encoder.pkl\n",
    "‚Ä¢ feature_names.pkl\n",
    "‚Ä¢ feature_registry.json\n",
    "\n",
    "DATA LEAKAGE PREVENTION:\n",
    "------------------------\n",
    "‚úì Train-test split done BEFORE any transformation\n",
    "‚úì Imputers fitted on train only, transformed both\n",
    "‚úì Encoder fitted on train only, transformed both\n",
    "‚úì No test set information leaked to training data\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Feature engineering complete - Ready for model training\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
