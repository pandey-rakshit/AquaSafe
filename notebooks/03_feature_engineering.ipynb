{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0479144c",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Feature Engineering & Encoding Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1c2e2",
   "metadata": {},
   "source": [
    "### **ðŸŽ¯ Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904f6d3",
   "metadata": {},
   "source": [
    "Convert the cleaned dataset into a model-ready feature matrix by encoding categorical variables and assembling numeric features without re-cleaning, scaling, or leakage. This notebook produces a stable features artifact for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778bc9c",
   "metadata": {},
   "source": [
    "### Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54904c1",
   "metadata": {},
   "source": [
    "###  Notebook\n",
    "\n",
    "**File:** `notebooks/03_feature_engineering.ipynb`\n",
    "\n",
    "**Stage:** Feature Engineering & Encoding\n",
    "\n",
    "**Input Contract:** `data/processed/nwmp_cleaned_v1.csv`\n",
    "\n",
    "**Output Contract:** `data/processed/nwmp_features_v1.(csv | parquet)`\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Transform the **cleaned and validated dataset** into a **model-ready feature matrix** by:\n",
    "\n",
    "* encoding categorical variables,\n",
    "* assembling numeric and engineered features,\n",
    "* enforcing strict schema invariants,\n",
    "\n",
    "**without performing cleaning, imputation, scaling, or modeling decisions.**\n",
    "\n",
    "This notebook establishes a **stable, reusable feature representation** for all downstream models.\n",
    "\n",
    "---\n",
    "\n",
    "### Input Assumptions (Contract Enforcement)\n",
    "\n",
    "The input dataset satisfies the following invariants:\n",
    "\n",
    "* âœ… No missing values (NaNs)\n",
    "* âœ… Leakage-prone, metadata, and sparse columns already removed\n",
    "* âœ… Numeric features properly typed\n",
    "* âœ… BDL (Below Detection Limit) information preserved as binary flags\n",
    "* âœ… One row = one observation\n",
    "* âœ… Target variable (`use_based_class`) present and clean\n",
    "\n",
    "If any of these conditions fail, the pipeline must return to **data cleaning**.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Engineering Scope\n",
    "\n",
    "#### Included Operations\n",
    "\n",
    "* Target separation (`X`, `y`)\n",
    "* Identification of categorical features\n",
    "* One-hot encoding of low-cardinality categorical columns\n",
    "* Assembly of numeric + encoded categorical features\n",
    "* Schema validation (row alignment, NaN checks)\n",
    "* Export of model-ready feature matrix\n",
    "\n",
    "#### Explicitly Excluded Operations\n",
    "\n",
    "* âŒ Data cleaning or imputation\n",
    "* âŒ Column dropping or leakage decisions\n",
    "* âŒ Feature scaling or normalization\n",
    "* âŒ Outlier handling\n",
    "* âŒ Feature selection\n",
    "* âŒ Model training\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Composition\n",
    "\n",
    "#### Numeric Features\n",
    "\n",
    "* Physicochemical parameters (pH, DO, conductivity, TDS, turbidity, etc.)\n",
    "* Chemical contaminants (nutrients, ions, hardness, alkalinity)\n",
    "* Biological indicators (fecal coliform, total coliform, streptococci)\n",
    "* Engineered **BDL indicator flags** (`*_is_bdl`)\n",
    "\n",
    "All numeric features are passed through **unchanged**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Categorical Features\n",
    "\n",
    "* Domain-relevant, low-cardinality features only\n",
    "* Encoded using **one-hot encoding**\n",
    "* No identifiers, metadata, or leakage-prone columns included\n",
    "\n",
    "High-cardinality or free-text columns are intentionally excluded upstream.\n",
    "\n",
    "---\n",
    "\n",
    "### Validation & Safety Checks\n",
    "\n",
    "Before export, the following conditions are enforced:\n",
    "\n",
    "* Feature matrix contains **no missing values**\n",
    "* Feature rows align exactly with target labels\n",
    "* No duplicate rows introduced during encoding\n",
    "\n",
    "These checks ensure downstream models receive a **stable and deterministic input**.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Artifact\n",
    "\n",
    "The notebook exports a single, versioned feature dataset:\n",
    "\n",
    "* **CSV (mandatory):** transparent, debuggable\n",
    "* **Parquet (optional):** optimized for performance and scale\n",
    "\n",
    "This artifact represents the **final feature contract** for all modeling notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "* **Separation of concerns:**\n",
    "  Cleaning â†’ Feature Engineering â†’ Modeling are strictly isolated.\n",
    "\n",
    "* **Reproducibility:**\n",
    "  Feature generation is deterministic and independent of model choice.\n",
    "\n",
    "* **Leakage safety:**\n",
    "  Only features that exist prior to labeling are included.\n",
    "\n",
    "* **Scalability:**\n",
    "  Same features can be reused across multiple models and experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### Status\n",
    "\n",
    "âœ… Feature engineering complete\n",
    "âœ… Schema validated\n",
    "âœ… Model-ready dataset exported\n",
    "ðŸŸ¢ **Ready for model training**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5dae5",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dae5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c999ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import DATA_DIR # Path to raw data source\n",
    "from src.data_preprocessing.create_dataframe import create_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae9265",
   "metadata": {},
   "source": [
    "### **ðŸ’¿ Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c574fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded cleaned dataset\n",
      "(171, 56)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING â€” CLEANED CONTRACT INPUT\n",
    "# ============================================================================\n",
    "\n",
    "INPUT_CSV = os.path.join(DATA_DIR, \"processed\", \"csv\", \"cleaned_water_quality_data.csv\")\n",
    "# Optional parquet if available:\n",
    "# INPUT_PARQUET = os.path.join(DATA_DIR, \"processed\", \"parquet\", \"cleaned_water_quality_data.parquet\")\n",
    "\n",
    "df = create_dataframe(INPUT_CSV)\n",
    "\n",
    "print(\"âœ“ Loaded cleaned dataset\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65aa14",
   "metadata": {},
   "source": [
    "### Input Contract Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd25f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” VALIDATING INPUT CONTRACT...\n",
      "================================================================================\n",
      "  âœ“ No missing values\n",
      "  âœ“ Target column 'use_based_class' present\n",
      "  âœ“ Target has no missing values\n",
      "  âœ“ Target classes validated: ['A', 'B', 'C', 'E']\n",
      "  âœ“ No duplicate rows\n",
      "  âœ“ Sufficient records: 171 (minimum: 50)\n",
      "  âœ“ Sufficient features: 56 (minimum: 10)\n",
      "  âœ“ Numeric features present: 29\n",
      "================================================================================\n",
      "\n",
      "âœ… ALL INPUT CONTRACT VALIDATIONS PASSED!\n",
      "   Dataset is ready for feature engineering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INPUT CONTRACT VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\nðŸ” VALIDATING INPUT CONTRACT...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_passed = True\n",
    "\n",
    "# Check 1: No missing values\n",
    "missing_count = df.isna().sum().sum()\n",
    "if missing_count == 0:\n",
    "    print(\"  âœ“ No missing values\")\n",
    "else:\n",
    "    print(f\"  âŒ FAILED: {missing_count} missing values found\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 2: Target exists and is clean\n",
    "TARGET_COL = \"use_based_class\"\n",
    "if TARGET_COL in df.columns:\n",
    "    print(f\"  âœ“ Target column '{TARGET_COL}' present\")\n",
    "    \n",
    "    target_missing = df[TARGET_COL].isna().sum()\n",
    "    if target_missing == 0:\n",
    "        print(f\"  âœ“ Target has no missing values\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAILED: Target has {target_missing} missing values\")\n",
    "        validation_passed = False\n",
    "else:\n",
    "    print(f\"  âŒ FAILED: Target column '{TARGET_COL}' not found\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 3: Verify expected classes\n",
    "expected_classes = {'A', 'B', 'C', 'E'}\n",
    "actual_classes = set(df[TARGET_COL].unique())\n",
    "\n",
    "if actual_classes == expected_classes:\n",
    "    print(f\"  âœ“ Target classes validated: {sorted(actual_classes)}\")\n",
    "elif actual_classes.issubset(expected_classes):\n",
    "    print(f\"  âš ï¸ WARNING: Only {len(actual_classes)} classes present: {sorted(actual_classes)}\")\n",
    "    print(f\"    Missing: {sorted(expected_classes - actual_classes)}\")\n",
    "else:\n",
    "    print(f\"  âŒ FAILED: Unexpected classes found\")\n",
    "    print(f\"    Expected: {sorted(expected_classes)}\")\n",
    "    print(f\"    Found: {sorted(actual_classes)}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 4: No duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "if dup_count == 0:\n",
    "    print(\"  âœ“ No duplicate rows\")\n",
    "else:\n",
    "    print(f\"  âŒ FAILED: {dup_count} duplicate rows found\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 5: Reasonable shape\n",
    "min_records = 50\n",
    "min_features = 10\n",
    "\n",
    "if df.shape[0] >= min_records:\n",
    "    print(f\"  âœ“ Sufficient records: {df.shape[0]} (minimum: {min_records})\")\n",
    "else:\n",
    "    print(f\"  âŒ FAILED: Only {df.shape[0]} records (minimum: {min_records})\")\n",
    "    validation_passed = False\n",
    "\n",
    "if df.shape[1] >= min_features:\n",
    "    print(f\"  âœ“ Sufficient features: {df.shape[1]} (minimum: {min_features})\")\n",
    "else:\n",
    "    print(f\"  âŒ FAILED: Only {df.shape[1]} features (minimum: {min_features})\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 6: Data types are appropriate\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    print(f\"  âœ“ Numeric features present: {len(numeric_cols)}\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ WARNING: No numeric features found\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"\\nâœ… ALL INPUT CONTRACT VALIDATIONS PASSED!\")\n",
    "    print(\"   Dataset is ready for feature engineering\\n\")\n",
    "else:\n",
    "    print(\"\\nâŒ INPUT CONTRACT VALIDATION FAILED!\")\n",
    "    print(\"   Please return to data cleaning notebook to fix issues\\n\")\n",
    "    raise ValueError(\"Input contract validation failed - cannot proceed with feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38937e",
   "metadata": {},
   "source": [
    "### **Define Target & Separate Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb89e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features / target separated\n",
      "X shape: (171, 55) | y shape: (171,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TARGET SEPARATION\n",
    "# ============================================================================\n",
    "\n",
    "TARGET_COL = \"use_based_class\"\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "print(\"âœ“ Features / target separated\")\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b2675",
   "metadata": {},
   "source": [
    "### Feature Engineering (Encoding Scope Only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c590044",
   "metadata": {},
   "source": [
    "**3.1 Identify categorical columns to encode**\n",
    "\n",
    "(Low-cardinality only; no identifiers, no leakage, no free-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b690f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to encode: ['type_water_body', 'river_basin', 'district', 'weather', 'approx_depth', 'human_activities', 'floating_matter', 'color', 'odor']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CATEGORICAL FEATURE SELECTION (ENCODING SCOPE)\n",
    "# ============================================================================\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "# Explicit exclusions (already decided in EDA / cleaning)\n",
    "EXCLUDE_CATS = [\n",
    "    # identifiers / metadata / free-text already removed upstream\n",
    "]\n",
    "\n",
    "# Keep only safe, low-cardinality categories\n",
    "encode_cats = [c for c in categorical_cols if c not in EXCLUDE_CATS]\n",
    "\n",
    "print(\"Categorical columns to encode:\", encode_cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139f4a5",
   "metadata": {},
   "source": [
    "### Categorical Feature Analysis\n",
    "\n",
    "**Objective:** Understand categorical features before encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1194ccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š CATEGORICAL FEATURES PRE-ENCODING ANALYSIS\n",
      "================================================================================\n",
      "Found 9 categorical features to encode:\n",
      "\n",
      "\n",
      "type_water_body:\n",
      "  Unique values: 7\n",
      "  Top 3 categories:\n",
      "    â€¢ River: 133 (77.8%)\n",
      "    â€¢ Creek: 18 (10.5%)\n",
      "    â€¢ Nala: 9 (5.3%)\n",
      "\n",
      "river_basin:\n",
      "  Unique values: 65\n",
      "  Top 3 categories:\n",
      "    â€¢ Godavari: 31 (18.1%)\n",
      "    â€¢ Godavri : 8 (4.7%)\n",
      "    â€¢ Patalganga : 7 (4.1%)\n",
      "  âš ï¸ WARNING: High cardinality (65) - will create 65 dummy columns\n",
      "     Consider target encoding or dropping this feature\n",
      "\n",
      "district:\n",
      "  Unique values: 20\n",
      "  Top 3 categories:\n",
      "    â€¢ Thane: 35 (20.5%)\n",
      "    â€¢ Raigad: 20 (11.7%)\n",
      "    â€¢ Nashik: 15 (8.8%)\n",
      "  â„¹ï¸ INFO: Medium cardinality (20) - will create 20 dummy columns\n",
      "\n",
      "weather:\n",
      "  Unique values: 3\n",
      "  Top 3 categories:\n",
      "    â€¢ Clear: 139 (81.3%)\n",
      "    â€¢ Raining: 19 (11.1%)\n",
      "    â€¢ Cloudy: 13 (7.6%)\n",
      "\n",
      "approx_depth:\n",
      "  Unique values: 3\n",
      "  Top 3 categories:\n",
      "    â€¢ Less than 50cm: 149 (87.1%)\n",
      "    â€¢ Greater than 100cm: 12 (7.0%)\n",
      "    â€¢ 50-100cm: 10 (5.8%)\n",
      "\n",
      "human_activities:\n",
      "  Unique values: 18\n",
      "  Top 3 categories:\n",
      "    â€¢ Others: 95 (55.6%)\n",
      "    â€¢ Bathing,Washing: 34 (19.9%)\n",
      "    â€¢ Bathing,Washing,Gardening,Others: 7 (4.1%)\n",
      "  â„¹ï¸ INFO: Medium cardinality (18) - will create 18 dummy columns\n",
      "\n",
      "floating_matter:\n",
      "  Unique values: 2\n",
      "  Top 3 categories:\n",
      "    â€¢ Yes: 150 (87.7%)\n",
      "    â€¢ No: 21 (12.3%)\n",
      "\n",
      "color:\n",
      "  Unique values: 9\n",
      "  Top 3 categories:\n",
      "    â€¢ Clear: 145 (84.8%)\n",
      "    â€¢ Light Brown: 17 (9.9%)\n",
      "    â€¢ Muddish: 2 (1.2%)\n",
      "\n",
      "odor:\n",
      "  Unique values: 6\n",
      "  Top 3 categories:\n",
      "    â€¢ unknown: 133 (77.8%)\n",
      "    â€¢ Odor Free: 30 (17.5%)\n",
      "    â€¢ organic: 3 (1.8%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Encoding Summary:\n",
      "   Categorical features to encode: 9\n",
      "   Total dummy columns to create: 133\n",
      "   Original feature count: 55\n",
      "   Expected feature count after encoding: 179\n",
      "\n",
      "   âš ï¸ WARNING: Encoding will create 133 dummy columns!\n",
      "      This may lead to:\n",
      "      â€¢ High dimensionality (curse of dimensionality)\n",
      "      â€¢ Increased memory usage\n",
      "      â€¢ Longer training times\n",
      "      Consider using alternative encoding methods\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CATEGORICAL FEATURE PRE-ENCODING ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\nðŸ“Š CATEGORICAL FEATURES PRE-ENCODING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(encode_cats) > 0:\n",
    "    print(f\"Found {len(encode_cats)} categorical features to encode:\\n\")\n",
    "    \n",
    "    for col in encode_cats:\n",
    "        n_unique = X[col].nunique()\n",
    "        n_missing = X[col].isna().sum()\n",
    "        top_values = X[col].value_counts().head(3)\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique values: {n_unique}\")\n",
    "        \n",
    "        if n_missing > 0:\n",
    "            print(f\"  âš ï¸ Missing: {n_missing} ({n_missing/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"  Top 3 categories:\")\n",
    "        for val, count in top_values.items():\n",
    "            pct = count / len(X) * 100\n",
    "            print(f\"    â€¢ {val}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Warning for high cardinality\n",
    "        if n_unique > 20:\n",
    "            print(f\"  âš ï¸ WARNING: High cardinality ({n_unique}) - will create {n_unique} dummy columns\")\n",
    "            print(f\"     Consider target encoding or dropping this feature\")\n",
    "        elif n_unique > 10:\n",
    "            print(f\"  â„¹ï¸ INFO: Medium cardinality ({n_unique}) - will create {n_unique} dummy columns\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_dummies = sum(X[col].nunique() for col in encode_cats)\n",
    "    print(f\"\\nðŸ“Š Encoding Summary:\")\n",
    "    print(f\"   Categorical features to encode: {len(encode_cats)}\")\n",
    "    print(f\"   Total dummy columns to create: {total_dummies}\")\n",
    "    print(f\"   Original feature count: {X.shape[1]}\")\n",
    "    print(f\"   Expected feature count after encoding: {X.shape[1] - len(encode_cats) + total_dummies}\")\n",
    "    \n",
    "    # Warning if dimension explosion\n",
    "    if total_dummies > 50:\n",
    "        print(f\"\\n   âš ï¸ WARNING: Encoding will create {total_dummies} dummy columns!\")\n",
    "        print(f\"      This may lead to:\")\n",
    "        print(f\"      â€¢ High dimensionality (curse of dimensionality)\")\n",
    "        print(f\"      â€¢ Increased memory usage\")\n",
    "        print(f\"      â€¢ Longer training times\")\n",
    "        print(f\"      Consider using alternative encoding methods\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No categorical features to encode\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca05483",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389e6ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Encoding complete\n",
      "Encoded shape: (171, 179)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ONE-HOT ENCODING (NO SCALING, NO LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "X_encoded = pd.get_dummies(\n",
    "    X,\n",
    "    columns=encode_cats,\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Encoding complete\")\n",
    "print(\"Encoded shape:\", X_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1249d",
   "metadata": {},
   "source": [
    "### âœ… Encoding Transformation Complete\n",
    "\n",
    "**Objective:** Document the impact of one-hot encoding on feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80fe7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š ENCODING TRANSFORMATION SUMMARY\n",
      "================================================================================\n",
      "Original Features:    55\n",
      "Encoded Features:     179\n",
      "Features Added:       124\n",
      "Categorical Encoded:  9\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Categorical Feature Expansion:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "type_water_body:\n",
      "  Original unique values: 7\n",
      "  Dummy columns created: 7\n",
      "  New columns:\n",
      "    â€¢ type_water_body_ Dam (n=1)\n",
      "    â€¢ type_water_body_Creek (n=18)\n",
      "    â€¢ type_water_body_Dam (n=3)\n",
      "    â€¢ type_water_body_Lake (n=1)\n",
      "    â€¢ type_water_body_Nala (n=9)\n",
      "    â€¢ type_water_body_River (n=133)\n",
      "    â€¢ type_water_body_Sea (n=6)\n",
      "\n",
      "river_basin:\n",
      "  Original unique values: 65\n",
      "  Dummy columns created: 65\n",
      "  â„¹ï¸ 65 columns created (too many to display)\n",
      "  First 5 columns: river_basin_Arnala , river_basin_BPT Navapur , river_basin_Bassein , river_basin_Bhagwati Bundar, river_basin_Bhatsa \n",
      "\n",
      "district:\n",
      "  Original unique values: 20\n",
      "  Dummy columns created: 20\n",
      "  â„¹ï¸ 20 columns created (too many to display)\n",
      "  First 5 columns: district_Akola, district_Amravati, district_Beed, district_Bhandara, district_Ch. Sambhaji Nagar\n",
      "\n",
      "weather:\n",
      "  Original unique values: 3\n",
      "  Dummy columns created: 3\n",
      "  New columns:\n",
      "    â€¢ weather_Clear (n=139)\n",
      "    â€¢ weather_Cloudy (n=13)\n",
      "    â€¢ weather_Raining (n=19)\n",
      "\n",
      "approx_depth:\n",
      "  Original unique values: 3\n",
      "  Dummy columns created: 3\n",
      "  New columns:\n",
      "    â€¢ approx_depth_50-100cm (n=10)\n",
      "    â€¢ approx_depth_Greater than 100cm (n=12)\n",
      "    â€¢ approx_depth_Less than 50cm (n=149)\n",
      "\n",
      "human_activities:\n",
      "  Original unique values: 18\n",
      "  Dummy columns created: 18\n",
      "  â„¹ï¸ 18 columns created (too many to display)\n",
      "  First 5 columns: human_activities_Bathing, human_activities_Bathing,Fishing, human_activities_Bathing,Fishing,Boating,Tourism Spot,Idol Emersion Wading, human_activities_Bathing,Fishing,Tourism Spot, human_activities_Bathing,Washing\n",
      "\n",
      "floating_matter:\n",
      "  Original unique values: 2\n",
      "  Dummy columns created: 2\n",
      "  New columns:\n",
      "    â€¢ floating_matter_No (n=21)\n",
      "    â€¢ floating_matter_Yes (n=150)\n",
      "\n",
      "color:\n",
      "  Original unique values: 9\n",
      "  Dummy columns created: 9\n",
      "  New columns:\n",
      "    â€¢ color_Blackish (n=1)\n",
      "    â€¢ color_Clear (n=145)\n",
      "    â€¢ color_Light Brown (n=17)\n",
      "    â€¢ color_Light Green (n=1)\n",
      "    â€¢ color_Muddish (n=2)\n",
      "    â€¢ color_Pale Yellow (n=1)\n",
      "    â€¢ color_Slightly Turbit (n=2)\n",
      "    â€¢ color_Turbit (n=1)\n",
      "    â€¢ color_slightly Turbit (n=1)\n",
      "\n",
      "odor:\n",
      "  Original unique values: 6\n",
      "  Dummy columns created: 6\n",
      "  New columns:\n",
      "    â€¢ odor_ Odourless (n=2)\n",
      "    â€¢ odor_Fishy (n=1)\n",
      "    â€¢ odor_Odor Free (n=30)\n",
      "    â€¢ odor_Odourless (n=2)\n",
      "    â€¢ odor_organic (n=3)\n",
      "    â€¢ odor_unknown (n=133)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Encoding complete - feature matrix ready for modeling\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENCODING IMPACT SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\nðŸ“Š ENCODING TRANSFORMATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Original Features:    {X.shape[1]}\")\n",
    "print(f\"Encoded Features:     {X_encoded.shape[1]}\")\n",
    "print(f\"Features Added:       {X_encoded.shape[1] - X.shape[1]}\")\n",
    "print(f\"Categorical Encoded:  {len(encode_cats)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if len(encode_cats) > 0:\n",
    "    print(\"\\nðŸ“‹ Categorical Feature Expansion:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for cat_col in encode_cats:\n",
    "        # Find new columns created from this categorical\n",
    "        new_cols = [col for col in X_encoded.columns if col.startswith(f\"{cat_col}_\")]\n",
    "        \n",
    "        print(f\"\\n{cat_col}:\")\n",
    "        print(f\"  Original unique values: {X[cat_col].nunique()}\")\n",
    "        print(f\"  Dummy columns created: {len(new_cols)}\")\n",
    "        \n",
    "        # Show column names if not too many\n",
    "        if len(new_cols) <= 10:\n",
    "            print(f\"  New columns:\")\n",
    "            for new_col in new_cols:\n",
    "                # Extract the category value from column name\n",
    "                category = new_col.replace(f\"{cat_col}_\", \"\")\n",
    "                count = X[cat_col].value_counts().get(category, 0)\n",
    "                print(f\"    â€¢ {new_col} (n={count})\")\n",
    "        else:\n",
    "            print(f\"  â„¹ï¸ {len(new_cols)} columns created (too many to display)\")\n",
    "            # Show just first 5\n",
    "            print(f\"  First 5 columns: {', '.join(new_cols[:5])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… Encoding complete - feature matrix ready for modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214f26f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ FINAL FEATURE MATRIX COMPOSITION\n",
      "================================================================================\n",
      "\n",
      "Total Features:          179\n",
      "  Numeric (continuous):  29\n",
      "  Binary/Boolean:        150\n",
      "  One-Hot Encoded:       133\n",
      "\n",
      "Feature Breakdown:\n",
      "  Original features:     55\n",
      "  Categorical encoded:   9 â†’ 133 dummies\n",
      "  Net change:            +124 features\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Œ Sample Features:\n",
      "\n",
      "Numeric (first 5): flow, temperature, dissolved_o2, ph, conductivity\n",
      "\n",
      "Binary (first 5): fecal_coliform_is_bdl, total_coliform_is_bdl, fecal_streptococci_is_bdl, total_kjeldahl_n_is_bdl, nitrate_n_is_bdl\n",
      "\n",
      "One-Hot Encoded (first 5): type_water_body_ Dam, type_water_body_Creek, type_water_body_Dam, type_water_body_Lake, type_water_body_Nala\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL FEATURE COMPOSITION\n",
    "# ============================================================================\n",
    "print(\"\\nðŸ“‹ FINAL FEATURE MATRIX COMPOSITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify feature types in encoded data\n",
    "numeric_features = X_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Identify binary/boolean features (including one-hot encoded)\n",
    "binary_features = []\n",
    "for col in X_encoded.columns:\n",
    "    if X_encoded[col].nunique() == 2:\n",
    "        binary_features.append(col)\n",
    "\n",
    "# One-hot encoded features\n",
    "one_hot_features = [col for col in X_encoded.columns if col not in X.columns]\n",
    "\n",
    "print(f\"\\nTotal Features:          {X_encoded.shape[1]}\")\n",
    "print(f\"  Numeric (continuous):  {len([f for f in numeric_features if f not in binary_features])}\")\n",
    "print(f\"  Binary/Boolean:        {len(binary_features)}\")\n",
    "print(f\"  One-Hot Encoded:       {len(one_hot_features)}\")\n",
    "\n",
    "print(f\"\\nFeature Breakdown:\")\n",
    "print(f\"  Original features:     {X.shape[1]}\")\n",
    "print(f\"  Categorical encoded:   {len(encode_cats)} â†’ {len(one_hot_features)} dummies\")\n",
    "print(f\"  Net change:            +{X_encoded.shape[1] - X.shape[1]} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Show sample features from each type\n",
    "print(\"\\nðŸ“Œ Sample Features:\")\n",
    "\n",
    "# Sample numeric (non-binary)\n",
    "sample_numeric = [f for f in numeric_features if f not in binary_features][:5]\n",
    "if sample_numeric:\n",
    "    print(f\"\\nNumeric (first 5): {', '.join(sample_numeric)}\")\n",
    "\n",
    "# Sample binary\n",
    "sample_binary = binary_features[:5]\n",
    "if sample_binary:\n",
    "    print(f\"\\nBinary (first 5): {', '.join(sample_binary)}\")\n",
    "\n",
    "# Sample one-hot\n",
    "sample_onehot = one_hot_features[:5]\n",
    "if sample_onehot:\n",
    "    print(f\"\\nOne-Hot Encoded (first 5): {', '.join(sample_onehot)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ac5ed",
   "metadata": {},
   "source": [
    "### Assemble Feature Matrix (No Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc1680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Feature matrix assembled and validated\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL FEATURE MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "X_features = X_encoded.copy()\n",
    "\n",
    "# Invariants\n",
    "assert X_features.isna().sum().sum() == 0, \"NaNs present after encoding\"\n",
    "assert X_features.shape[0] == y.shape[0], \"Row mismatch between X and y\"\n",
    "\n",
    "print(\"âœ“ Feature matrix assembled and validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2aec1",
   "metadata": {},
   "source": [
    "### Export Model-Ready Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ac18033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features exported to /Users/rex/Documents/personal/AquaSafe/data/processed/csv/nwmp_features_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPORT FEATURES (MODEL-READY, UN-SCALED)\n",
    "# ============================================================================\n",
    "\n",
    "OUT_CSV = os.path.join(DATA_DIR, \"processed\", \"csv\", \"nwmp_features_v1.csv\")\n",
    "OUT_PARQUET = os.path.join(DATA_DIR, \"processed\", \"parquet\", \"nwmp_features_v1.parquet\")\n",
    "\n",
    "X_features.assign(**{TARGET_COL: y}).to_csv(OUT_CSV, index=False)\n",
    "\n",
    "try:\n",
    "    X_features.assign(**{TARGET_COL: y}).to_parquet(OUT_PARQUET, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Parquet export skipped:\", e)\n",
    "\n",
    "print(f\"âœ“ Features exported to {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecdb94",
   "metadata": {},
   "source": [
    "### ðŸ” Output Contract Validation\n",
    "\n",
    "**Objective:** Verify that exported dataset meets quality standards and is ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3e5e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” VALIDATING OUTPUT CONTRACT...\n",
      "================================================================================\n",
      "  âœ“ File successfully exported and reloaded: /Users/rex/Documents/personal/AquaSafe/data/processed/csv/nwmp_features_v1.csv\n",
      "  âœ“ Row count preserved: 171\n",
      "  âœ“ Column count correct: 180 (features + target)\n",
      "  âœ“ Target column 'use_based_class' present\n",
      "  âœ“ Target classes preserved: 4 classes\n",
      "  âœ“ No missing values in exported file\n",
      "  âœ“ Numeric columns preserved: 29\n",
      "  âœ“ No duplicate rows\n",
      "================================================================================\n",
      "\n",
      "âœ… ALL OUTPUT CONTRACT VALIDATIONS PASSED!\n",
      "âœ… Model-ready dataset exported: /Users/rex/Documents/personal/AquaSafe/data/processed/csv/nwmp_features_v1.csv\n",
      "   Records: 171\n",
      "   Features: 179 (+ 1 target)\n",
      "   Size: 175.84 KB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OUTPUT CONTRACT VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\nðŸ” VALIDATING OUTPUT CONTRACT...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_passed = True\n",
    "\n",
    "# Reload the exported file to verify\n",
    "try:\n",
    "    df_verify = pd.read_csv(OUT_CSV)\n",
    "    print(f\"  âœ“ File successfully exported and reloaded: {OUT_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ FAILED: Could not reload exported file\")\n",
    "    print(f\"     Error: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "if validation_passed:\n",
    "    # Check 1: Shape preserved\n",
    "    expected_rows = df.shape[0]\n",
    "    expected_cols = X_encoded.shape[1] + 1  # +1 for target\n",
    "    \n",
    "    if df_verify.shape[0] == expected_rows:\n",
    "        print(f\"  âœ“ Row count preserved: {df_verify.shape[0]}\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAILED: Row count mismatch\")\n",
    "        print(f\"     Expected: {expected_rows}, Got: {df_verify.shape[0]}\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    if df_verify.shape[1] == expected_cols:\n",
    "        print(f\"  âœ“ Column count correct: {df_verify.shape[1]} (features + target)\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAILED: Column count mismatch\")\n",
    "        print(f\"     Expected: {expected_cols}, Got: {df_verify.shape[1]}\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check 2: Target preserved\n",
    "    if TARGET_COL in df_verify.columns:\n",
    "        print(f\"  âœ“ Target column '{TARGET_COL}' present\")\n",
    "        \n",
    "        if df_verify[TARGET_COL].nunique() == y.nunique():\n",
    "            print(f\"  âœ“ Target classes preserved: {df_verify[TARGET_COL].nunique()} classes\")\n",
    "        else:\n",
    "            print(f\"  âŒ FAILED: Target classes lost\")\n",
    "            print(f\"     Expected: {y.nunique()}, Got: {df_verify[TARGET_COL].nunique()}\")\n",
    "            validation_passed = False\n",
    "    else:\n",
    "        print(f\"  âŒ FAILED: Target column missing from export\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check 3: No NaNs introduced\n",
    "    nan_count = df_verify.isna().sum().sum()\n",
    "    if nan_count == 0:\n",
    "        print(\"  âœ“ No missing values in exported file\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAILED: {nan_count} NaN values found in exported file\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check 4: Data types preserved\n",
    "    numeric_count = df_verify.select_dtypes(include=[np.number]).shape[1]\n",
    "    if numeric_count > 0:\n",
    "        print(f\"  âœ“ Numeric columns preserved: {numeric_count}\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAILED: No numeric columns in export\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check 5: No duplicates introduced\n",
    "    dup_count = df_verify.duplicated().sum()\n",
    "    if dup_count == 0:\n",
    "        print(\"  âœ“ No duplicate rows\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ WARNING: {dup_count} duplicate rows found\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"\\nâœ… ALL OUTPUT CONTRACT VALIDATIONS PASSED!\")\n",
    "    print(f\"âœ… Model-ready dataset exported: {OUT_CSV}\")\n",
    "    print(f\"   Records: {df_verify.shape[0]}\")\n",
    "    print(f\"   Features: {df_verify.shape[1] - 1} (+ 1 target)\")\n",
    "    print(f\"   Size: {os.path.getsize(OUT_CSV) / 1024:.2f} KB\")\n",
    "else:\n",
    "    print(\"\\nâŒ OUTPUT CONTRACT VALIDATION FAILED!\")\n",
    "    print(\"   Please review the export process\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405292ed",
   "metadata": {},
   "source": [
    "### ðŸ“ Feature Registry\n",
    "\n",
    "**Objective:** Create a machine-readable manifest of all features for deployment and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4231751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Creating Feature Registry...\n",
      "âœ“ Feature registry saved: /Users/rex/Documents/personal/AquaSafe/data/processed/feature_registry_v1.json\n",
      "\n",
      "ðŸ“‹ Feature Registry Summary:\n",
      "================================================================================\n",
      "Version:           1.0\n",
      "Total Features:    179\n",
      "Target:            use_based_class\n",
      "Classes:           A, B, C, E\n",
      "Encoding Method:   one_hot\n",
      "Registry Location: /Users/rex/Documents/personal/AquaSafe/data/processed/feature_registry_v1.json\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Œ First 20 Features:\n",
      "    1. flow\n",
      "    2. temperature\n",
      "    3. dissolved_o2\n",
      "    4. ph\n",
      "    5. conductivity\n",
      "    6. bod\n",
      "    7. nitrate_n\n",
      "    8. fecal_coliform\n",
      "    9. total_coliform\n",
      "   10. fecal_streptococci\n",
      "   11. turbidity\n",
      "   12. phenophelene_alkanity\n",
      "   13. total_alkalinity\n",
      "   14. chlorides\n",
      "   15. cod\n",
      "   16. total_kjeldahl_n\n",
      "   17. amonia_n\n",
      "   18. hardness_caco3\n",
      "   19. calcium_caco3\n",
      "   20. magnesium_caco3\n",
      "   ... and 159 more\n",
      "\n",
      "âœ… Feature engineering complete and documented!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE REGISTRY (FOR DOCUMENTATION & DEPLOYMENT)\n",
    "# ============================================================================\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nðŸ“ Creating Feature Registry...\")\n",
    "\n",
    "# Create feature registry\n",
    "feature_registry = {\n",
    "    \"metadata\": {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": \"feature_engineering_pipeline\",\n",
    "        \"input_file\": INPUT_CSV,\n",
    "        \"output_file\": OUT_CSV\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"total_records\": len(df),\n",
    "        \"total_features\": X_encoded.shape[1],\n",
    "        \"target_name\": TARGET_COL,\n",
    "        \"target_classes\": sorted(y.unique().tolist())\n",
    "    },\n",
    "    \"feature_details\": {\n",
    "        \"feature_names\": X_encoded.columns.tolist(),\n",
    "        \"numeric_features\": len([f for f in numeric_features if f not in binary_features]),\n",
    "        \"binary_features\": len(binary_features),\n",
    "        \"one_hot_encoded_features\": len(one_hot_features)\n",
    "    },\n",
    "    \"encoding_strategy\": {\n",
    "        \"method\": \"one_hot\",\n",
    "        \"drop_first\": False,\n",
    "        \"categorical_columns_encoded\": encode_cats,\n",
    "        \"dummy_columns_created\": len(one_hot_features)\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"missing_values\": 0,\n",
    "        \"duplicate_rows\": 0,\n",
    "        \"data_types\": {\n",
    "            \"numeric\": len(numeric_features),\n",
    "            \"categorical\": len(encode_cats)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save feature registry\n",
    "registry_path = os.path.join(DATA_DIR, \"processed\", \"feature_registry_v1.json\")\n",
    "with open(registry_path, 'w') as f:\n",
    "    json.dump(feature_registry, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Feature registry saved: {registry_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nðŸ“‹ Feature Registry Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Version:           {feature_registry['metadata']['version']}\")\n",
    "print(f\"Total Features:    {feature_registry['dataset_info']['total_features']}\")\n",
    "print(f\"Target:            {feature_registry['dataset_info']['target_name']}\")\n",
    "print(f\"Classes:           {', '.join(feature_registry['dataset_info']['target_classes'])}\")\n",
    "print(f\"Encoding Method:   {feature_registry['encoding_strategy']['method']}\")\n",
    "print(f\"Registry Location: {registry_path}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Optionally display first 20 feature names\n",
    "print(f\"\\nðŸ“Œ First 20 Features:\")\n",
    "for i, feat in enumerate(X_encoded.columns[:20], 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "if len(X_encoded.columns) > 20:\n",
    "    print(f\"   ... and {len(X_encoded.columns) - 20} more\")\n",
    "\n",
    "print(\"\\nâœ… Feature engineering complete and documented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03e0f7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
